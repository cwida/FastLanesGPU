==PROF== Connected to process 3738 (/home/ubuntu/fff/cmake-build-release-g4dn/gpu/crystal-opt/src/crystal_opt_q21)
Using device 0: Tesla V100-SXM2-16GB (PTX version 700, SM700, 80 SMs, 15754 free / 16151 total MB physmem, 898.048 GB/s @ 877000 kHz mem clock, ECC on)
==PROF== Profiling "build_hashtable_s" - 0: 0%....50%....100% - 73 passes
==PROF== Profiling "build_hashtable_p" - 1: 0%....50%....100% - 74 passes
==PROF== Profiling "build_hashtable_d" - 2: 0%....50%....100% - 73 passes
==PROF== Profiling "probe" - 3: 0%....50%....100% - 74 passes
1992 40 6574868694
1993 40 6952043914
1994 40 6525239576
1995 40 6764559245
1996 40 6725548424
1997 40 6596102991
1998 40 3988851825
1992 41 7047701749
1993 41 6909841940
1994 41 6978800980
1995 41 7036474627
1996 41 7233045193
1997 41 6938053628
1998 41 4065391978
1992 42 6450484539
1993 42 6886094182
1994 42 6852294265
1995 42 6749813918
1996 42 6568551778
1997 42 6845017761
1998 42 3773836113
1992 43 6918393482
1993 43 6621428714
1994 43 7068738463
1995 43 6820930145
1996 43 6762634261
1997 43 6849537060
1998 43 3882704011
1992 44 6343659176
1993 44 6094791212
1994 44 6661136530
1995 44 6085276694
1996 44 6176324016
1997 44 6315911460
1998 44 3925731952
1992 45 6499025385
1993 45 6779833973
1994 45 6435942251
1995 45 6738626764
1996 45 6763207154
1997 45 6889101910
1998 45 3879170338
1992 46 6833102567
1993 46 7017493760
1994 46 7015998639
1995 46 6897957727
1996 46 6948998143
1997 46 6510502742
1998 46 3911656234
1992 47 6922095842
1993 47 7061777324
1994 47 6877252420
1995 47 6575484550
1996 47 6517266740
1997 47 6651228318
1998 47 3835254989
1992 48 6818173454
1993 48 6961952133
1994 48 7051587760
1995 48 7329421356
1996 48 7164243172
1997 48 7052687209
1998 48 4132526586
1992 49 6907633511
1993 49 6614194460
1994 49 6773107666
1995 49 6954065693
1996 49 6747336514
1997 49 6947116463
1998 49 3906763122
1992 50 7098282117
1993 50 7263350231
1994 50 7199754789
1995 50 7246399314
1996 50 6860318803
1997 50 7184653230
1998 50 4293359981
1992 51 7474015795
1993 51 7031859249
1994 51 6749353264
1995 51 7395439319
1996 51 7118371952
1997 51 7427932834
1998 51 4080129102
1992 52 7001985495
1993 52 6734276751
1994 52 6965715192
1995 52 6934765252
1996 52 6895454124
1997 52 6802928999
1998 52 3916065107
1992 53 6531087764
1993 53 6258171804
1994 53 6197787972
1995 53 6605279401
1996 53 6722321819
1997 53 6879971631
1998 53 3561102555
1992 54 7041216650
1993 54 6601732879
1994 54 6737632272
1995 54 6483760392
1996 54 6778740509
1997 54 6950964366
1998 54 3960525994
1992 55 7034325953
1993 55 7070112383
1994 55 6835473512
1995 55 6681873420
1996 55 6755919599
1997 55 6883879790
1998 55 3842444977
1992 56 6672842875
1993 56 6362926487
1994 56 6787572691
1995 56 6941448166
1996 56 6349041382
1997 56 6831022793
1998 56 3750580610
1992 57 6762940511
1993 57 6200194110
1994 57 6360354225
1995 57 6799718937
1996 57 6500504812
1997 57 6464594869
1998 57 3690857660
1992 58 6367358727
1993 58 6519991362
1994 58 6228367674
1995 58 6522760927
1996 58 6043428578
1997 58 6386892483
1998 58 3888948778
1992 59 6542091138
1993 59 6669384898
1994 59 6566921738
1995 59 6725584633
1996 59 6678854924
1997 59 6518974991
1998 59 3661443815
1992 60 7397021390
1993 60 6985315570
1994 60 7171226221
1995 60 7409511342
1996 60 7217054942
1997 60 7241219598
1998 60 4134876965
1992 61 6439487815
1993 61 6190501096
1994 61 6658242784
1995 61 6300444895
1996 61 6394989839
1997 61 6372986872
1998 61 3692782928
1992 62 7142709582
1993 62 6575099186
1994 62 6577906605
1995 62 6758016505
1996 62 6713821475
1997 62 7061699626
1998 62 3911733232
1992 63 6684932832
1993 63 6784872415
1994 63 6771692541
1995 63 6832689629
1996 63 6769695502
1997 63 6801959247
1998 63 3916910435
1992 64 6403427844
1993 64 6686657397
1994 64 6560285004
1995 64 6654877138
1996 64 6403809726
1997 64 6364910756
1998 64 3757788047
1992 65 6800534485
1993 65 6932192888
1994 65 6599703796
1995 65 6950320978
1996 65 6745507185
1997 65 6965554062
1998 65 3856421228
1992 66 6608507118
1993 66 6720022834
1994 66 7249477139
1995 66 6982989122
1996 66 6895681155
1997 66 7131587724
1998 66 4050936159
1992 67 6789994724
1993 67 7034832635
1994 67 6533866956
1995 67 7089400123
1996 67 6950690822
1997 67 6872602250
1998 67 3798832673
1992 68 6761138392
1993 68 7117328614
1994 68 7003067656
1995 68 6916376148
1996 68 6810961498
1997 68 6421432868
1998 68 4365901362
1992 69 6333970291
1993 69 6591672386
1994 69 6491372066
1995 69 6759048824
1996 69 6636341404
1997 69 6396375726
1998 69 3755850783
1992 70 6863351080
1993 70 7236349480
1994 70 7065985619
1995 70 6799040388
1996 70 7281402064
1997 70 6735307561
1998 70 4062655575
1992 71 6978088606
1993 71 6615095404
1994 71 6642491845
1995 71 7135465638
1996 71 6904578270
1997 71 6886861519
1998 71 3971062487
1992 72 6077239048
1993 72 6379459453
1994 72 6452415472
1995 72 6170313509
1996 72 5916688379
1997 72 5963369350
1998 72 3683718797
1992 73 6671048755
1993 73 6565112476
1994 73 6641285247
1995 73 6887663633
1996 73 6439642020
1997 73 6675192946
1998 73 3814007830
1992 74 6999195521
1993 74 7007686388
1994 74 6670519880
1995 74 6744064671
1996 74 6614217057
1997 74 6523268368
1998 74 4023666133
1992 75 6627416528
1993 75 6758016664
1994 75 6751975322
1995 75 7047693486
1996 75 6567430366
1997 75 6781762704
1998 75 4063152322
1992 76 6785625804
1993 76 6930340135
1994 76 6382873777
1995 76 6206415993
1996 76 6805542040
1997 76 6422414358
1998 76 4087738859
1992 77 6848387744
1993 77 6623249454
1994 77 6588036917
1995 77 6589295276
1996 77 6603676047
1997 77 6383121125
1998 77 4063691471
1992 78 6240883199
1993 78 6551226256
1994 78 6647824791
1995 78 6494311762
1996 78 6358269587
1997 78 6349078074
1998 78 3890548095
1992 79 6948601533
1993 79 7058895576
1994 79 7280306702
1995 79 7174749606
1996 79 7134521672
1997 79 7009756092
1998 79 4233289127
Res Count: 280
Time Taken Total: 2577.93
{"query":21,"time_query":2577.8}
==PROF== Disconnected from process 3738
[3738] crystal_opt_q21@127.0.0.1
  void build_hashtable_s<(int)128, (int)4>(int *, int *, int, int *, int) (40, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       720.80
    SM Frequency            cycle/nsecond         1.07
    Elapsed Cycles                  cycle         4696
    Memory Throughput                   %         7.37
    DRAM Throughput                     %         7.37
    Duration                      usecond         4.38
    L1/TEX Cache Throughput             %         6.32
    L2 Cache Throughput                 %         2.78
    SM Active Cycles                cycle         1421
    Compute (SM) Throughput             %         1.86
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.23
    Executed Ipc Elapsed  inst/cycle         0.07
    Issue Slots Busy               %         6.15
    Issued Ipc Active     inst/cycle         0.25
    SM Busy                        %         6.15
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 96.08%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ------------ ------------
    Metric Name        Metric Unit Metric Value
    ----------------- ------------ ------------
    Memory Throughput Gbyte/second        54.38
    Mem Busy                     %         2.55
    Max Bandwidth                %         7.37
    L1/TEX Hit Rate              %            0
    L2 Hit Rate                  %         3.99
    Mem Pipes Busy               %         0.59
    ----------------- ------------ ------------

    Section: Memory Workload Analysis Chart
    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  
          additional metric could enable the rule to provide more guidance.                                             

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         6.34
    Issued Warp Per Scheduler                        0.06
    No Eligible                            %        93.66
    Active Warps Per Scheduler          warp         1.00
    Eligible Warps Per Scheduler        warp         0.06
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.63%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 15.8 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        15.77
    Warp Cycles Per Executed Instruction           cycle        16.63
    Avg. Active Threads Per Warp                                16.20
    Avg. Not Predicated Off Threads Per Warp                    15.19
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 31.03%                                                                                          
          On average, each warp of this kernel spends 4.9 cycles being stalled waiting for an immediate constant cache  
          (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;        
          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      
          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    
          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       
          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   
          threads of a warp access the same location, then constant memory can be as fast as a register access. This    
          stall type represents about 31.0% of the total average of 15.8 cycles between issuing two instructions.       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.9793%                                                                                         
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 16.2 threads being active per cycle. This is further reduced    
          to 15.2 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst        82.96
    Executed Instructions                           inst        26546
    Avg. Issued Instructions Per Scheduler          inst        87.46
    Issued Instructions                             inst        27986
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     40
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size            byte               0
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            5120
    Waves Per SM                                                0.03
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          The grid for this launch is configured to execute only 40 blocks, which is less than the GPU's 80             
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.09
    Achieved Active Warps Per SM           warp         3.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 92.63%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst         2876
    Branch Efficiency                   %        58.57
    Avg. Divergent Branches                       1.47
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 7.14%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 1441 excessive sectors (20% of the total  
          7183 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void build_hashtable_p<(int)128, (int)4>(int *, int *, int *, int, int *, int) (1563, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       744.82
    SM Frequency            cycle/nsecond         1.11
    Elapsed Cycles                  cycle        22266
    Memory Throughput                   %        75.85
    DRAM Throughput                     %        75.85
    Duration                      usecond        20.06
    L1/TEX Cache Throughput             %        21.13
    L2 Cache Throughput                 %        26.67
    SM Active Cycles                cycle     19529.47
    Compute (SM) Throughput             %        13.70
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.62
    Executed Ipc Elapsed  inst/cycle         0.54
    Issue Slots Busy               %        15.59
    Issued Ipc Active     inst/cycle         0.62
    SM Busy                        %        15.59
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.81%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ------------ ------------
    Metric Name        Metric Unit Metric Value
    ----------------- ------------ ------------
    Memory Throughput Gbyte/second       578.50
    Mem Busy                     %        26.67
    Max Bandwidth                %        75.85
    L1/TEX Hit Rate              %         0.01
    L2 Hit Rate                  %         8.83
    Mem Pipes Busy               %         6.97
    ----------------- ------------ ------------

    Section: Memory Workload Analysis Chart
    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  
          additional metric could enable the rule to provide more guidance.                                             

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.607%                                                                                          
          The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 1.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        15.78
    Issued Warp Per Scheduler                        0.16
    No Eligible                            %        84.22
    Active Warps Per Scheduler          warp        12.15
    Eligible Warps Per Scheduler        warp         0.27
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 24.15%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 6.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          12.15 active warps per scheduler, but only an average of 0.27 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        76.99
    Warp Cycles Per Executed Instruction           cycle        77.61
    Avg. Active Threads Per Warp                                16.02
    Avg. Not Predicated Off Threads Per Warp                    15.64
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.15%                                                                                          
          On average, each warp of this kernel spends 39.1 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 50.7% of the total average of 77.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.002%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 16.0 threads being active per cycle. This is further reduced    
          to 15.6 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      3019.96
    Executed Instructions                           inst       966388
    Avg. Issued Instructions Per Scheduler          inst      3044.49
    Issued Instructions                             inst       974237
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1563
    Registers Per Thread             register/thread              18
    Shared Memory Configuration Size            byte               0
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          200064
    Waves Per SM                                                1.22
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 1 full waves and a partial wave of 283 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, the partial wave may account for   
          up to 50.0% of the total kernel runtime with a lower occupancy of 24.3%. Try launching a grid with no         
          partial wave. The overall impact of this tail effect also lessens with the number of full waves executed for  
          a grid. See the Hardware Model                                                                                
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        75.69
    Achieved Active Warps Per SM           warp        48.44
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 24.15%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (75.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.12
    Branch Instructions              inst       118258
    Branch Efficiency                   %        72.76
    Avg. Divergent Branches                      42.58
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 5.813%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 23924 excessive sectors (7% of the total  
          360344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.    
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void build_hashtable_d<(int)128, (int)4>(int *, int *, int, int *, int, int) (5, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       671.64
    SM Frequency            cycle/usecond       997.12
    Elapsed Cycles                  cycle         4279
    Memory Throughput                   %         1.51
    DRAM Throughput                     %         1.51
    Duration                      usecond         4.29
    L1/TEX Cache Throughput             %        25.60
    L2 Cache Throughput                 %         1.32
    SM Active Cycles                cycle       157.47
    Compute (SM) Throughput             %         0.22
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.22
    Executed Ipc Elapsed  inst/cycle         0.01
    Issue Slots Busy               %         5.88
    Issued Ipc Active     inst/cycle         0.24
    SM Busy                        %         5.88
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.86%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ------------ ------------
    Metric Name        Metric Unit Metric Value
    ----------------- ------------ ------------
    Memory Throughput Gbyte/second        10.37
    Mem Busy                     %         0.84
    Max Bandwidth                %         1.51
    L1/TEX Hit Rate              %         5.21
    L2 Hit Rate                  %        41.57
    Mem Pipes Busy               %         0.16
    ----------------- ------------ ------------

    Section: Memory Workload Analysis Chart
    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  
          additional metric could enable the rule to provide more guidance.                                             

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.02194%                                                                                        
          The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses   
          4.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between       
          threads, results in 9.2 sectors per request, or 9.2*32 = 293.6 bytes of cache data transfers per request.     
          The optimal thread address pattern for 4.0 byte accesses would result in 4.0*32 = 128.0 bytes of cache data   
          transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for             
          uncoalesced global stores.                                                                                    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.11%                                                                                           
          The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.5 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         6.11
    Issued Warp Per Scheduler                        0.06
    No Eligible                            %        93.89
    Active Warps Per Scheduler          warp         0.99
    Eligible Warps Per Scheduler        warp         0.06
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.89%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 16.4 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        16.25
    Warp Cycles Per Executed Instruction           cycle        17.30
    Avg. Active Threads Per Warp                                31.93
    Avg. Not Predicated Off Threads Per Warp                    28.35
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         8.69
    Executed Instructions                           inst         2782
    Avg. Issued Instructions Per Scheduler          inst         9.26
    Issued Instructions                             inst         2962
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      5
    Registers Per Thread             register/thread              30
    Shared Memory Configuration Size            byte               0
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             640
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 93.75%                                                                                          
          The grid for this launch is configured to execute only 5 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.09
    Achieved Active Warps Per SM           warp         3.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 93.89%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.06
    Branch Instructions              inst          166
    Branch Efficiency                   %        97.73
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 6.883%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 812 excessive sectors (39% of the total   
          2092 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void probe<(int)128, (int)4>(int *, int *, int *, int *, int, int *, int, int *, int, int *, int, int *) (117161, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.0
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/usecond       873.75
    SM Frequency            cycle/nsecond         1.30
    Elapsed Cycles                  cycle      1531013
    Memory Throughput                   %        92.86
    DRAM Throughput                     %        92.86
    Duration                      msecond         1.17
    L1/TEX Cache Throughput             %        52.19
    L2 Cache Throughput                 %        44.12
    SM Active Cycles                cycle   1528368.90
    Compute (SM) Throughput             %        34.55
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 2:1. The kernel achieved 0% of  
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.38
    Executed Ipc Elapsed  inst/cycle         1.38
    Issue Slots Busy               %        34.61
    Issued Ipc Active     inst/cycle         1.38
    SM Busy                        %        34.61
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (24.1%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ----------------- ------------ ------------
    Metric Name        Metric Unit Metric Value
    ----------------- ------------ ------------
    Memory Throughput Gbyte/second       830.81
    Mem Busy                     %        44.12
    Max Bandwidth                %        92.86
    L1/TEX Hit Rate              %        54.53
    L2 Hit Rate                  %        47.53
    Mem Pipes Busy               %        10.29
    ----------------- ------------ ------------

    Section: Memory Workload Analysis Chart
    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  
          additional metric could enable the rule to provide more guidance.                                             

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 2.223%                                                                                          
          The memory access pattern for global loads in L1TEX might not be optimal. On average, this kernel accesses    
          4.9 bytes per thread per memory request; but the address pattern, possibly caused by the stride between       
          threads, results in 9.8 sectors per request, or 9.8*32 = 313.9 bytes of cache data transfers per request.     
          The optimal thread address pattern for 4.9 byte accesses would result in 4.9*32 = 158.1 bytes of cache data   
          transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for             
          uncoalesced global loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 28.23%                                                                                          
          The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 1.3 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.7841%                                                                                         
          The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 1.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 50.52%                                                                                          
          The memory access pattern for loads from device memory causes 30,456,682 sectors to be read from DRAM, which  
          is 1.4x of the 21,645,747 sectors which cause a miss in the L2 cache. The DRAM fetch granularity for read     
          misses in L2 is 64 bytes, i.e. the lower or upper half of an L2 cache line. Try changing your access pattern  
          to make use of both sectors returned by a DRAM read request for optimal usage of the DRAM throughput. For     
          strided memory reads, avoid strides of 64 bytes or larger to avoid moving unused sectors from DRAM to L2.     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        34.63
    Issued Warp Per Scheduler                        0.35
    No Eligible                            %        65.37
    Active Warps Per Scheduler          warp        14.34
    Eligible Warps Per Scheduler        warp         0.63
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.143%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 2.9 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 16 warps per scheduler, this kernel allocates an average of   
          14.34 active warps per scheduler, but only an average of 0.63 warps were eligible per cycle. Eligible warps   
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        41.42
    Warp Cycles Per Executed Instruction           cycle        41.42
    Avg. Active Threads Per Warp                                14.38
    Avg. Not Predicated Off Threads Per Warp                    12.49
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.143%                                                                                          
          On average, each warp of this kernel spends 34.0 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 82.1% of the total average of 41.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 21.07%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 14.4 threads being active per cycle. This is further reduced    
          to 12.5 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    528865.90
    Executed Instructions                           inst    169237087
    Avg. Issued Instructions Per Scheduler          inst    528913.22
    Issued Instructions                             inst    169252229
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 117161
    Registers Per Thread             register/thread              29
    Shared Memory Configuration Size            byte               0
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        14996608
    Waves Per SM                                               91.53
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           32
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           16
    Theoretical Active Warps per SM        warp           64
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        89.67
    Achieved Active Warps Per SM           warp        57.39
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.143%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (89.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.10
    Branch Instructions              inst     16750526
    Branch Efficiency                   %        66.51
    Avg. Divergent Branches                    7113.95
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 73.2%                                                                                           
          This kernel has uncoalesced global accesses resulting in a total of 65007931 excessive sectors (73% of the    
          total 88620164 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

