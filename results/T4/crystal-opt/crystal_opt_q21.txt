==PROF== Connected to process 21469 (/home/ubuntu/fff/cmake-build-release-g4dn/gpu/crystal-opt/src/crystal_opt_q21)
Using device 0: Tesla T4 (PTX version 750, SM750, 40 SMs, 14802 free / 14929 total MB physmem, 320.064 GB/s @ 5001000 kHz mem clock, ECC on)
==PROF== Profiling "build_hashtable_s" - 0: 0%....50%....100% - 35 passes
==PROF== Profiling "build_hashtable_p" - 1: 0%....50%....100% - 35 passes
==PROF== Profiling "build_hashtable_d" - 2: 0%....50%....100% - 35 passes
==PROF== Profiling "probe" - 3: 0%....50%....100% - 35 passes
1992 40 620792484
1993 40 696134057
1994 40 646183449
1995 40 683789210
1996 40 510952202
1997 40 601329805
1998 40 310212681
1992 41 619072460
1993 41 641906797
1994 41 600001881
1995 41 538237825
1996 41 626948592
1997 41 599768514
1998 41 324844587
1992 42 490930542
1993 42 590494981
1994 42 637973270
1995 42 563917939
1996 42 489891798
1997 42 575414713
1998 42 382795528
1992 43 746180561
1993 43 667809574
1994 43 582681681
1995 43 622468901
1996 43 615832436
1997 43 665181553
1998 43 391879175
1992 44 592643829
1993 44 547150428
1994 44 687417394
1995 44 557813495
1996 44 539509100
1997 44 591240347
1998 44 339362761
1992 45 666818456
1993 45 614006733
1994 45 553771550
1995 45 718580318
1996 45 604069580
1997 45 582504582
1998 45 418644887
1992 46 497930218
1993 46 603429138
1994 46 570930370
1995 46 705815966
1996 46 595735717
1997 46 652582336
1998 46 460900222
1992 47 573950112
1993 47 581793154
1994 47 555531717
1995 47 684622773
1996 47 655735847
1997 47 644209268
1998 47 368241619
1992 48 629436211
1993 48 732890335
1994 48 616859338
1995 48 687199268
1996 48 741147488
1997 48 755567624
1998 48 436528223
1992 49 665878691
1993 49 584135775
1994 49 589016154
1995 49 607002298
1996 49 563497671
1997 49 545937355
1998 49 309175941
1992 50 553577586
1993 50 555705138
1994 50 676062441
1995 50 641710612
1996 50 575388404
1997 50 578436711
1998 50 314285647
1992 51 656700076
1993 51 627130013
1994 51 591728980
1995 51 657535469
1996 51 658819802
1997 51 654743943
1998 51 391790954
1992 52 571366086
1993 52 610589229
1994 52 544138184
1995 52 529390478
1996 52 612212648
1997 52 520737479
1998 52 253877981
1992 53 532838175
1993 53 556392755
1994 53 506771410
1995 53 567384149
1996 53 471880515
1997 53 589317682
1998 53 286129583
1992 54 575100090
1993 54 598048745
1994 54 669399089
1995 54 673188844
1996 54 607298942
1997 54 682448220
1998 54 396578150
1992 55 575644119
1993 55 538626597
1994 55 641068147
1995 55 683443283
1996 55 628222285
1997 55 578103277
1998 55 348596079
1992 56 562595481
1993 56 572869443
1994 56 523516106
1995 56 534863977
1996 56 536099358
1997 56 590451889
1998 56 317284773
1992 57 514802952
1993 57 493315679
1994 57 599287565
1995 57 596024828
1996 57 615338121
1997 57 598601936
1998 57 308349815
1992 58 543346708
1993 58 604020487
1994 58 515506085
1995 58 599834564
1996 58 517842408
1997 58 608170121
1998 58 341434816
1992 59 491786465
1993 59 655668497
1994 59 655183200
1995 59 584917742
1996 59 559185452
1997 59 576734822
1998 59 326633797
1992 60 602373460
1993 60 615880897
1994 60 643804380
1995 60 713302883
1996 60 623220244
1997 60 680711137
1998 60 354376769
1992 61 633928973
1993 61 565901926
1994 61 647661017
1995 61 647289672
1996 61 637768457
1997 61 593124378
1998 61 380715354
1992 62 554701238
1993 62 565208933
1994 62 718895078
1995 62 609303895
1996 62 691969792
1997 62 631016696
1998 62 358310182
1992 63 546043513
1993 63 660789968
1994 63 655833720
1995 63 702057957
1996 63 653344348
1997 63 550179447
1998 63 419353251
1992 64 647297520
1993 64 582390534
1994 64 529474222
1995 64 560461020
1996 64 591003083
1997 64 564085649
1998 64 398848738
1992 65 799379105
1993 65 576848715
1994 65 636493983
1995 65 713329066
1996 65 633922964
1997 65 684284629
1998 65 335073096
1992 66 795689545
1993 66 759898311
1994 66 697404326
1995 66 693856011
1996 66 605367841
1997 66 682817524
1998 66 372528215
1992 67 648627112
1993 67 649305965
1994 67 543254019
1995 67 737599852
1996 67 646443167
1997 67 703348298
1998 67 356128002
1992 68 608555802
1993 68 573449583
1994 68 610859739
1995 68 628687768
1996 68 689535294
1997 68 638125635
1998 68 387752384
1992 69 616083074
1993 69 603750123
1994 69 566272871
1995 69 693347954
1996 69 621193535
1997 69 569915068
1998 69 371569162
1992 70 627072231
1993 70 554942415
1994 70 736308788
1995 70 589463137
1996 70 701770686
1997 70 561626445
1998 70 378101727
1992 71 710702139
1993 71 699720829
1994 71 666578569
1995 71 576221762
1996 71 535280597
1997 71 628168690
1998 71 405952025
1992 72 595000850
1993 72 596286095
1994 72 590361006
1995 72 665356177
1996 72 595905720
1997 72 624894859
1998 72 373008532
1992 73 525131548
1993 73 594835274
1994 73 586002871
1995 73 533249668
1996 73 585914955
1997 73 478354667
1998 73 362991667
1992 74 662752933
1993 74 632459703
1994 74 662533721
1995 74 709515121
1996 74 661386832
1997 74 611544878
1998 74 322587523
1992 75 518515033
1993 75 601539100
1994 75 551806661
1995 75 601270873
1996 75 600967332
1997 75 466864598
1998 75 274934885
1992 76 775269032
1993 76 773058041
1994 76 730757877
1995 76 675467554
1996 76 814445849
1997 76 780193108
1998 76 423598818
1992 77 633158355
1993 77 713278246
1994 77 620767558
1995 77 634874801
1996 77 704631788
1997 77 639898919
1998 77 392782198
1992 78 512420094
1993 78 523847906
1994 78 529135579
1995 78 578182924
1996 78 511907354
1997 78 570804688
1998 78 370584319
1992 79 709935019
1993 79 539722143
1994 79 596339358
1995 79 697095930
1996 79 783593202
1997 79 652546402
1998 79 435709816
Res Count: 280
Time Taken Total: 25185.4
{"query":21,"time_query":25185.4}
==PROF== Disconnected from process 21469
[21469] crystal_opt_q21@127.0.0.1
  void build_hashtable_s<(int)128, (int)4>(int *, int *, int, int *, int) (63, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         4.79
    SM Frequency            cycle/usecond       556.02
    Elapsed Cycles                  cycle         3811
    Memory Throughput                   %        14.42
    DRAM Throughput                     %        14.42
    Duration                      usecond         6.85
    L1/TEX Cache Throughput             %        12.70
    L2 Cache Throughput                 %         5.09
    SM Active Cycles                cycle      1615.80
    Compute (SM) Throughput             %         3.19
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.26
    Executed Ipc Elapsed  inst/cycle         0.11
    Issue Slots Busy               %         7.40
    Issued Ipc Active     inst/cycle         0.30
    SM Busy                        %         7.51
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 96.02%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ------------ ------------
    Metric Name        Metric Unit Metric Value
    ----------------- ------------ ------------
    Memory Throughput Gbyte/second        44.15
    Mem Busy                     %         5.09
    Max Bandwidth                %        14.42
    L1/TEX Hit Rate              %            0
    L2 Hit Rate                  %        12.48
    Mem Pipes Busy               %         3.05
    ----------------- ------------ ------------

    Section: Memory Workload Analysis Chart
    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  
          additional metric could enable the rule to provide more guidance.                                             

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         6.92
    Issued Warp Per Scheduler                        0.07
    No Eligible                            %        93.08
    Active Warps Per Scheduler          warp         1.47
    Eligible Warps Per Scheduler        warp         0.08
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 85.58%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 14.4 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    
          1.47 active warps per scheduler, but only an average of 0.08 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons on the Warp  
          State Statistics and Source Counters sections.                                                                

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        21.25
    Warp Cycles Per Executed Instruction           cycle        24.10
    Avg. Active Threads Per Warp                                29.44
    Avg. Not Predicated Off Threads Per Warp                    29.28
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       105.38
    Executed Instructions                           inst        16861
    Avg. Issued Instructions Per Scheduler          inst       119.51
    Issued Instructions                             inst        19121
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     63
    Registers Per Thread             register/thread              20
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            8064
    Waves Per SM                                                0.20
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        19.06
    Achieved Active Warps Per SM           warp         6.10
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 80.94%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (19.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.21
    Branch Instructions              inst         3603
    Branch Efficiency                   %        97.30
    Avg. Divergent Branches                       0.30
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.4533%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 136 excessive sectors (2% of the total    
          8209 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void build_hashtable_p<(int)128, (int)4>(int *, int *, int *, int, int *, int) (1954, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         4.48
    SM Frequency            cycle/usecond       525.07
    Elapsed Cycles                  cycle        30400
    Memory Throughput                   %        85.69
    DRAM Throughput                     %        85.69
    Duration                      usecond        57.89
    L1/TEX Cache Throughput             %        33.22
    L2 Cache Throughput                 %        27.53
    SM Active Cycles                cycle     28782.75
    Compute (SM) Throughput             %        18.21
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.61
    Executed Ipc Elapsed  inst/cycle         0.58
    Issue Slots Busy               %        15.40
    Issued Ipc Active     inst/cycle         0.62
    SM Busy                        %        15.40
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.47%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ------------ ------------
    Metric Name        Metric Unit Metric Value
    ----------------- ------------ ------------
    Memory Throughput Gbyte/second       245.55
    Mem Busy                     %        27.53
    Max Bandwidth                %        85.69
    L1/TEX Hit Rate              %         0.00
    L2 Hit Rate                  %         2.28
    Mem Pipes Busy               %        18.21
    ----------------- ------------ ------------

    Section: Memory Workload Analysis Chart
    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  
          additional metric could enable the rule to provide more guidance.                                             

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.3676%                                                                                         
          The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 1.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        13.80
    Issued Warp Per Scheduler                        0.14
    No Eligible                            %        86.20
    Active Warps Per Scheduler          warp         6.61
    Eligible Warps Per Scheduler        warp         0.18
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 14.31%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 7.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    
          6.61 active warps per scheduler, but only an average of 0.18 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        47.92
    Warp Cycles Per Executed Instruction           cycle        48.44
    Avg. Active Threads Per Warp                                26.34
    Avg. Not Predicated Off Threads Per Warp                    26.28
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.31%                                                                                          
          On average, each warp of this kernel spends 21.3 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 44.5% of the total average of 47.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.31%                                                                                          
          On average, each warp of this kernel spends 19.5 cycles being stalled after EXIT waiting for all outstanding  
          memory operations to complete so that warp's resources can be freed. A high number of stalls due to draining  
          warps typically occurs when a lot of data is written to memory towards the end of a kernel. Make sure the     
          memory access patterns of these store operations are optimal for the target architecture and consider         
          parallelized data reduction, if applicable. This stall type represents about 40.8% of the total average of    
          47.9 cycles between issuing two instructions.                                                                 
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      4385.65
    Executed Instructions                           inst       701704
    Avg. Issued Instructions Per Scheduler          inst      4433.79
    Issued Instructions                             inst       709406
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1954
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          250112
    Waves Per SM                                                6.11
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.99
    Achieved Active Warps Per SM           warp        29.44
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.18
    Branch Instructions              inst       129629
    Branch Efficiency                   %        94.56
    Avg. Divergent Branches                      21.28
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 1.409%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 5904 excessive sectors (2% of the total   
          390018 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.    
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void build_hashtable_d<(int)128, (int)4>(int *, int *, int, int *, int, int) (5, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         4.68
    SM Frequency            cycle/usecond       551.85
    Elapsed Cycles                  cycle         3216
    Memory Throughput                   %         3.84
    DRAM Throughput                     %         3.84
    Duration                      usecond         5.82
    L1/TEX Cache Throughput             %        30.39
    L2 Cache Throughput                 %         2.13
    SM Active Cycles                cycle       251.72
    Compute (SM) Throughput             %         0.57
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.28
    Executed Ipc Elapsed  inst/cycle         0.02
    Issue Slots Busy               %         7.31
    Issued Ipc Active     inst/cycle         0.29
    SM Busy                        %         7.31
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.01%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ------------ ------------
    Metric Name        Metric Unit Metric Value
    ----------------- ------------ ------------
    Memory Throughput Gbyte/second        11.51
    Mem Busy                     %         1.72
    Max Bandwidth                %         3.84
    L1/TEX Hit Rate              %         4.36
    L2 Hit Rate                  %        49.69
    Mem Pipes Busy               %         0.53
    ----------------- ------------ ------------

    Section: Memory Workload Analysis Chart
    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  
          additional metric could enable the rule to provide more guidance.                                             

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.05677%                                                                                        
          The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses   
          4.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between       
          threads, results in 9.1 sectors per request, or 9.1*32 = 290.4 bytes of cache data transfers per request.     
          The optimal thread address pattern for 4.0 byte accesses would result in 4.0*32 = 128.0 bytes of cache data   
          transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for             
          uncoalesced global stores.                                                                                    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.1415%                                                                                         
          The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.8 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         7.07
    Issued Warp Per Scheduler                        0.07
    No Eligible                            %        92.93
    Active Warps Per Scheduler          warp         0.94
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.93%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 14.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    
          0.94 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        13.26
    Warp Cycles Per Executed Instruction           cycle        14.04
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    28.58
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst        17.39
    Executed Instructions                           inst         2782
    Avg. Issued Instructions Per Scheduler          inst        18.41
    Issued Instructions                             inst         2946
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      5
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             640
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 87.5%                                                                                           
          The grid for this launch is configured to execute only 5 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.18
    Achieved Active Warps Per SM           warp         3.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 87.82%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (12.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.06
    Branch Instructions              inst          166
    Branch Efficiency                   %        97.73
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.765%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 812 excessive sectors (39% of the total   
          2092 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void probe<(int)128, (int)4>(int *, int *, int *, int *, int, int *, int, int *, int, int *, int, int *) (187479, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         4.98
    SM Frequency            cycle/usecond       582.87
    Elapsed Cycles                  cycle      1015929
    Memory Throughput                   %        87.95
    DRAM Throughput                     %        87.95
    Duration                      msecond         1.74
    L1/TEX Cache Throughput             %        48.14
    L2 Cache Throughput                 %        29.43
    SM Active Cycles                cycle   1013809.50
    Compute (SM) Throughput             %        56.91
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.28
    Executed Ipc Elapsed  inst/cycle         2.28
    Issue Slots Busy               %        57.03
    Issued Ipc Active     inst/cycle         2.28
    SM Busy                        %        57.03
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (47.9%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ----------------- ------------ ------------
    Metric Name        Metric Unit Metric Value
    ----------------- ------------ ------------
    Memory Throughput Gbyte/second       280.18
    Mem Busy                     %        29.43
    Max Bandwidth                %        87.95
    L1/TEX Hit Rate              %        37.85
    L2 Hit Rate                  %         8.77
    Mem Pipes Busy               %        36.99
    ----------------- ------------ ------------

    Section: Memory Workload Analysis Chart
    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  
          additional metric could enable the rule to provide more guidance.                                             

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 6.416%                                                                                          
          The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 3.1 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.09378%                                                                                        
          The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 1.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 80.85%                                                                                          
          The memory access pattern for loads from device memory causes 13,016,122 sectors to be read from DRAM, which  
          is 1.0x of the 12,757,527 sectors which cause a miss in the L2 cache. The DRAM fetch granularity for read     
          misses in L2 is 64 bytes, i.e. the lower or upper half of an L2 cache line. Try changing your access pattern  
          to make use of both sectors returned by a DRAM read request for optimal usage of the DRAM throughput. For     
          strided memory reads, avoid strides of 64 bytes or larger to avoid moving unused sectors from DRAM to L2.     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        56.83
    Issued Warp Per Scheduler                        0.57
    No Eligible                            %        43.17
    Active Warps Per Scheduler          warp         6.46
    Eligible Warps Per Scheduler        warp         1.18
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 12.05%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 1.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    
          6.46 active warps per scheduler, but only an average of 1.18 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        11.37
    Warp Cycles Per Executed Instruction           cycle        11.37
    Avg. Active Threads Per Warp                                28.80
    Avg. Not Predicated Off Threads Per Warp                    25.56
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.05%                                                                                          
          On average, each warp of this kernel spends 4.9 cycles being stalled waiting for a scoreboard dependency on a 
          L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited upon  
          to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the memory      
          access patterns are optimal for the target architecture, attempt to increase cache hit rates by increasing    
          data locality (coalescing), or by changing the cache configuration. Consider moving frequently used data to   
          shared memory. This stall type represents about 43.5% of the total average of 11.4 cycles between issuing     
          two instructions.                                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    578107.31
    Executed Instructions                           inst     92497169
    Avg. Issued Instructions Per Scheduler          inst    578158.86
    Issued Instructions                             inst     92505418
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 187479
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        23997312
    Waves Per SM                                              585.87
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        82.08
    Achieved Active Warps Per SM           warp        26.27
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.05%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (82.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.06
    Branch Instructions              inst      5150651
    Branch Efficiency                   %        89.24
    Avg. Divergent Branches                    1391.69
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 27.04%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 6096381 excessive sectors (27% of the     
          total 22464399 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source       
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

