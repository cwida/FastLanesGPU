==PROF== Connected to process 24377 (/home/ubuntu/fff/cmake-build-release-g4dn/gpu/crystal/src/crystal_q21)
Using device 0: Tesla T4 (PTX version 750, SM750, 40 SMs, 14802 free / 14929 total MB physmem, 320.064 GB/s @ 5001000 kHz mem clock, ECC on)
==PROF== Profiling "build_hashtable_s" - 0: 0%....50%....100% - 35 passes
==PROF== Profiling "build_hashtable_p" - 1: 0%....50%....100% - 35 passes
==PROF== Profiling "build_hashtable_d" - 2: 0%....50%....100% - 35 passes
==PROF== Profiling "probe" - 3: 0%....50%....100% - 35 passes
1992 40 6574868694
1993 40 6952043914
1994 40 6525239576
1995 40 6764559245
1996 40 6725548424
1997 40 6596102991
1998 40 3988851825
1992 41 7047701749
1993 41 6909841940
1994 41 6978800980
1995 41 7036474627
1996 41 7233045193
1997 41 6938053628
1998 41 4065391978
1992 42 6450484539
1993 42 6886094182
1994 42 6852294265
1995 42 6749813918
1996 42 6568551778
1997 42 6845017761
1998 42 3773836113
1992 43 6918393482
1993 43 6621428714
1994 43 7068738463
1995 43 6820930145
1996 43 6762634261
1997 43 6849537060
1998 43 3882704011
1992 44 6343659176
1993 44 6094791212
1994 44 6661136530
1995 44 6085276694
1996 44 6176324016
1997 44 6315911460
1998 44 3925731952
1992 45 6499025385
1993 45 6779833973
1994 45 6435942251
1995 45 6738626764
1996 45 6763207154
1997 45 6889101910
1998 45 3879170338
1992 46 6833102567
1993 46 7017493760
1994 46 7015998639
1995 46 6897957727
1996 46 6948998143
1997 46 6510502742
1998 46 3911656234
1992 47 6922095842
1993 47 7061777324
1994 47 6877252420
1995 47 6575484550
1996 47 6517266740
1997 47 6651228318
1998 47 3835254989
1992 48 6818173454
1993 48 6961952133
1994 48 7051587760
1995 48 7329421356
1996 48 7164243172
1997 48 7052687209
1998 48 4132526586
1992 49 6907633511
1993 49 6614194460
1994 49 6773107666
1995 49 6954065693
1996 49 6747336514
1997 49 6947116463
1998 49 3906763122
1992 50 7098282117
1993 50 7263350231
1994 50 7199754789
1995 50 7246399314
1996 50 6860318803
1997 50 7184653230
1998 50 4293359981
1992 51 7474015795
1993 51 7031859249
1994 51 6749353264
1995 51 7395439319
1996 51 7118371952
1997 51 7427932834
1998 51 4080129102
1992 52 7001985495
1993 52 6734276751
1994 52 6965715192
1995 52 6934765252
1996 52 6895454124
1997 52 6802928999
1998 52 3916065107
1992 53 6531087764
1993 53 6258171804
1994 53 6197787972
1995 53 6605279401
1996 53 6722321819
1997 53 6879971631
1998 53 3561102555
1992 54 7041216650
1993 54 6601732879
1994 54 6737632272
1995 54 6483760392
1996 54 6778740509
1997 54 6950964366
1998 54 3960525994
1992 55 7034325953
1993 55 7070112383
1994 55 6835473512
1995 55 6681873420
1996 55 6755919599
1997 55 6883879790
1998 55 3842444977
1992 56 6672842875
1993 56 6362926487
1994 56 6787572691
1995 56 6941448166
1996 56 6349041382
1997 56 6831022793
1998 56 3750580610
1992 57 6762940511
1993 57 6200194110
1994 57 6360354225
1995 57 6799718937
1996 57 6500504812
1997 57 6464594869
1998 57 3690857660
1992 58 6367358727
1993 58 6519991362
1994 58 6228367674
1995 58 6522760927
1996 58 6043428578
1997 58 6386892483
1998 58 3888948778
1992 59 6542091138
1993 59 6669384898
1994 59 6566921738
1995 59 6725584633
1996 59 6678854924
1997 59 6518974991
1998 59 3661443815
1992 60 7397021390
1993 60 6985315570
1994 60 7171226221
1995 60 7409511342
1996 60 7217054942
1997 60 7241219598
1998 60 4134876965
1992 61 6439487815
1993 61 6190501096
1994 61 6658242784
1995 61 6300444895
1996 61 6394989839
1997 61 6372986872
1998 61 3692782928
1992 62 7142709582
1993 62 6575099186
1994 62 6577906605
1995 62 6758016505
1996 62 6713821475
1997 62 7061699626
1998 62 3911733232
1992 63 6684932832
1993 63 6784872415
1994 63 6771692541
1995 63 6832689629
1996 63 6769695502
1997 63 6801959247
1998 63 3916910435
1992 64 6403427844
1993 64 6686657397
1994 64 6560285004
1995 64 6654877138
1996 64 6403809726
1997 64 6364910756
1998 64 3757788047
1992 65 6800534485
1993 65 6932192888
1994 65 6599703796
1995 65 6950320978
1996 65 6745507185
1997 65 6965554062
1998 65 3856421228
1992 66 6608507118
1993 66 6720022834
1994 66 7249477139
1995 66 6982989122
1996 66 6895681155
1997 66 7131587724
1998 66 4050936159
1992 67 6789994724
1993 67 7034832635
1994 67 6533866956
1995 67 7089400123
1996 67 6950690822
1997 67 6872602250
1998 67 3798832673
1992 68 6761138392
1993 68 7117328614
1994 68 7003067656
1995 68 6916376148
1996 68 6810961498
1997 68 6421432868
1998 68 4365901362
1992 69 6333970291
1993 69 6591672386
1994 69 6491372066
1995 69 6759048824
1996 69 6636341404
1997 69 6396375726
1998 69 3755850783
1992 70 6863351080
1993 70 7236349480
1994 70 7065985619
1995 70 6799040388
1996 70 7281402064
1997 70 6735307561
1998 70 4062655575
1992 71 6978088606
1993 71 6615095404
1994 71 6642491845
1995 71 7135465638
1996 71 6904578270
1997 71 6886861519
1998 71 3971062487
1992 72 6077239048
1993 72 6379459453
1994 72 6452415472
1995 72 6170313509
1996 72 5916688379
1997 72 5963369350
1998 72 3683718797
1992 73 6671048755
1993 73 6565112476
1994 73 6641285247
1995 73 6887663633
1996 73 6439642020
1997 73 6675192946
1998 73 3814007830
1992 74 6999195521
1993 74 7007686388
1994 74 6670519880
1995 74 6744064671
1996 74 6614217057
1997 74 6523268368
1998 74 4023666133
1992 75 6627416528
1993 75 6758016664
1994 75 6751975322
1995 75 7047693486
1996 75 6567430366
1997 75 6781762704
1998 75 4063152322
1992 76 6785625804
1993 76 6930340135
1994 76 6382873777
1995 76 6206415993
1996 76 6805542040
1997 76 6422414358
1998 76 4087738859
1992 77 6848387744
1993 77 6623249454
1994 77 6588036917
1995 77 6589295276
1996 77 6603676047
1997 77 6383121125
1998 77 4063691471
1992 78 6240883199
1993 78 6551226256
1994 78 6647824791
1995 78 6494311762
1996 78 6358269587
1997 78 6349078074
1998 78 3890548095
1992 79 6948601533
1993 79 7058895576
1994 79 7280306702
1995 79 7174749606
1996 79 7134521672
1997 79 7009756092
1998 79 4233289127
Res Count: 280
Time Taken Total: 24889.7
{"query":21,"time_query":24889.7}
==PROF== Disconnected from process 24377
[24377] crystal_q21@127.0.0.1
  void build_hashtable_s<(int)128, (int)4>(int *, int *, int, int *, int) (40, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         4.79
    SM Frequency            cycle/usecond       557.22
    Elapsed Cycles                  cycle         3699
    Memory Throughput                   %        15.18
    DRAM Throughput                     %        15.18
    Duration                      usecond         6.62
    L1/TEX Cache Throughput             %         8.28
    L2 Cache Throughput                 %         4.68
    SM Active Cycles                cycle      2378.93
    Compute (SM) Throughput             %         4.66
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.27
    Executed Ipc Elapsed  inst/cycle         0.18
    Issue Slots Busy               %         7.23
    Issued Ipc Active     inst/cycle         0.29
    SM Busy                        %         7.23
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 95.4%                                                                                     
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ------------ ------------
    Metric Name        Metric Unit Metric Value
    ----------------- ------------ ------------
    Memory Throughput Gbyte/second        46.56
    Mem Busy                     %         4.68
    Max Bandwidth                %        15.18
    L1/TEX Hit Rate              %            0
    L2 Hit Rate                  %        13.69
    Mem Pipes Busy               %         2.79
    ----------------- ------------ ------------

    Section: Memory Workload Analysis Chart
    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  
          additional metric could enable the rule to provide more guidance.                                             

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         7.08
    Issued Warp Per Scheduler                        0.07
    No Eligible                            %        92.92
    Active Warps Per Scheduler          warp         0.95
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.82%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 14.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    
          0.95 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        13.47
    Warp Cycles Per Executed Instruction           cycle        14.22
    Avg. Active Threads Per Warp                                15.91
    Avg. Not Predicated Off Threads Per Warp                    15.07
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.464%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 15.9 threads being active per cycle. This is further reduced    
          to 15.1 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       162.91
    Executed Instructions                           inst        26066
    Avg. Issued Instructions Per Scheduler          inst       171.91
    Issued Instructions                             inst        27506
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                     40
    Registers Per Thread             register/thread              20
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread            5120
    Waves Per SM                                                0.12
    -------------------------------- --------------- ---------------

    OPT   If you execute __syncthreads() to synchronize the threads of a block, it is recommended to have more than the 
          achieved 1 blocks per multiprocessor. This way, blocks that aren't waiting for __syncthreads() can keep the   
          hardware busy.                                                                                                

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.25
    Achieved Active Warps Per SM           warp         3.92
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 84.82%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (12.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst         2876
    Branch Efficiency                   %        58.57
    Avg. Divergent Branches                       2.93
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 7.978%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 1441 excessive sectors (20% of the total  
          7183 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void build_hashtable_p<(int)128, (int)4>(int *, int *, int *, int, int *, int) (1563, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         4.57
    SM Frequency            cycle/usecond       534.54
    Elapsed Cycles                  cycle        32009
    Memory Throughput                   %        79.90
    DRAM Throughput                     %        79.90
    Duration                      usecond        59.87
    L1/TEX Cache Throughput             %        27.26
    L2 Cache Throughput                 %        24.16
    SM Active Cycles                cycle     30276.90
    Compute (SM) Throughput             %        19.29
    ----------------------- ------------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.81
    Executed Ipc Elapsed  inst/cycle         0.77
    Issue Slots Busy               %        20.39
    Issued Ipc Active     inst/cycle         0.82
    SM Busy                        %        20.39
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 87.92%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ------------ ------------
    Metric Name        Metric Unit Metric Value
    ----------------- ------------ ------------
    Memory Throughput Gbyte/second       233.51
    Mem Busy                     %        24.16
    Max Bandwidth                %        79.90
    L1/TEX Hit Rate              %         0.02
    L2 Hit Rate                  %         8.79
    Mem Pipes Busy               %        18.39
    ----------------- ------------ ------------

    Section: Memory Workload Analysis Chart
    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  
          additional metric could enable the rule to provide more guidance.                                             

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.395%                                                                                          
          The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 1.2 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        18.76
    Issued Warp Per Scheduler                        0.19
    No Eligible                            %        81.24
    Active Warps Per Scheduler          warp         6.71
    Eligible Warps Per Scheduler        warp         0.25
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 20.1%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 5.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    
          6.71 active warps per scheduler, but only an average of 0.25 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        35.74
    Warp Cycles Per Executed Instruction           cycle        36.02
    Avg. Active Threads Per Warp                                15.82
    Avg. Not Predicated Off Threads Per Warp                    15.65
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 20.1%                                                                                           
          On average, each warp of this kernel spends 20.1 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 56.3% of the total average of 35.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.855%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 15.8 threads being active per cycle. This is further reduced    
          to 15.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      6125.61
    Executed Instructions                           inst       980097
    Avg. Issued Instructions Per Scheduler          inst      6173.99
    Issued Instructions                             inst       987839
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   1563
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread          200064
    Waves Per SM                                                4.88
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        91.38
    Achieved Active Warps Per SM           warp        29.24
    ------------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.12
    Branch Instructions              inst       118258
    Branch Efficiency                   %        72.76
    Avg. Divergent Branches                      85.17
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 6.204%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 23924 excessive sectors (7% of the total  
          360344 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.    
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void build_hashtable_d<(int)128, (int)4>(int *, int *, int, int *, int, int) (5, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         4.75
    SM Frequency            cycle/usecond       557.35
    Elapsed Cycles                  cycle         3247
    Memory Throughput                   %         3.80
    DRAM Throughput                     %         3.80
    Duration                      usecond         5.82
    L1/TEX Cache Throughput             %        30.56
    L2 Cache Throughput                 %         2.11
    SM Active Cycles                cycle       250.85
    Compute (SM) Throughput             %         0.57
    ----------------------- ------------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.28
    Executed Ipc Elapsed  inst/cycle         0.02
    Issue Slots Busy               %         7.34
    Issued Ipc Active     inst/cycle         0.29
    SM Busy                        %         7.34
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 95%                                                                                       
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ------------ ------------
    Metric Name        Metric Unit Metric Value
    ----------------- ------------ ------------
    Memory Throughput Gbyte/second        11.55
    Mem Busy                     %         1.70
    Max Bandwidth                %         3.80
    L1/TEX Hit Rate              %         4.61
    L2 Hit Rate                  %        50.64
    Mem Pipes Busy               %         0.52
    ----------------- ------------ ------------

    Section: Memory Workload Analysis Chart
    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  
          additional metric could enable the rule to provide more guidance.                                             

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.05669%                                                                                        
          The memory access pattern for global stores in L1TEX might not be optimal. On average, this kernel accesses   
          4.0 bytes per thread per memory request; but the address pattern, possibly caused by the stride between       
          threads, results in 9.1 sectors per request, or 9.1*32 = 290.4 bytes of cache data transfers per request.     
          The optimal thread address pattern for 4.0 byte accesses would result in 4.0*32 = 128.0 bytes of cache data   
          transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for             
          uncoalesced global stores.                                                                                    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.1433%                                                                                         
          The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 2.8 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         7.07
    Issued Warp Per Scheduler                        0.07
    No Eligible                            %        92.93
    Active Warps Per Scheduler          warp         0.93
    Eligible Warps Per Scheduler        warp         0.07
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 92.93%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 14.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    
          0.93 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        13.22
    Warp Cycles Per Executed Instruction           cycle        14.00
    Avg. Active Threads Per Warp                                31.92
    Avg. Not Predicated Off Threads Per Warp                    28.58
    ---------------------------------------- ----------- ------------

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst        17.39
    Executed Instructions                           inst         2782
    Avg. Issued Instructions Per Scheduler          inst        18.41
    Issued Instructions                             inst         2946
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      5
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread             640
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 87.5%                                                                                           
          The grid for this launch is configured to execute only 5 blocks, which is less than the GPU's 40              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        12.18
    Achieved Active Warps Per SM           warp         3.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 87.82%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (12.2%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.06
    Branch Instructions              inst          166
    Branch Efficiency                   %        97.73
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.801%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 812 excessive sectors (39% of the total   
          2092 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void probe<(int)128, (int)4>(int *, int *, int *, int *, int, int *, int, int *, int, int *, int, int *) (117161, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 7.5
    Section: GPU Speed Of Light Throughput
    ----------------------- ------------- ------------
    Metric Name               Metric Unit Metric Value
    ----------------------- ------------- ------------
    DRAM Frequency          cycle/nsecond         4.99
    SM Frequency            cycle/usecond       584.58
    Elapsed Cycles                  cycle      4476549
    Memory Throughput                   %        96.40
    DRAM Throughput                     %        96.40
    Duration                      msecond         7.66
    L1/TEX Cache Throughput             %        65.76
    L2 Cache Throughput                 %        40.98
    SM Active Cycles                cycle   4474138.67
    Compute (SM) Throughput             %        23.88
    ----------------------- ------------- ------------

    INF   The kernel is utilizing greater than 80.0% of the available compute or memory performance of the device. To   
          further improve performance, work will likely need to be shifted from the most utilized to another unit.      
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 32:1. The kernel achieved 0% of 
          this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide       
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.96
    Executed Ipc Elapsed  inst/cycle         0.96
    Issue Slots Busy               %        23.89
    Issued Ipc Active     inst/cycle         0.96
    SM Busy                        %        23.89
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 84.24%                                                                                    
          All compute pipelines are under-utilized. Either this kernel is very small or it doesn't issue enough warps   
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ----------------- ------------ ------------
    Metric Name        Metric Unit Metric Value
    ----------------- ------------ ------------
    Memory Throughput Gbyte/second       308.00
    Mem Busy                     %        40.98
    Max Bandwidth                %        96.40
    L1/TEX Hit Rate              %        18.41
    L2 Hit Rate                  %        53.82
    Mem Pipes Busy               %        15.02
    ----------------- ------------ ------------

    Section: Memory Workload Analysis Chart
    WRN   The optional metric lts__average_gcomp_input_sector_success_rate.pct could not be found. Collecting it as an  
          additional metric could enable the rule to provide more guidance.                                             

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.444%                                                                                          
          The memory access pattern for global loads in L1TEX might not be optimal. On average, this kernel accesses    
          4.8 bytes per thread per memory request; but the address pattern, possibly caused by the stride between       
          threads, results in 8.3 sectors per request, or 8.3*32 = 266.7 bytes of cache data transfers per request.     
          The optimal thread address pattern for 4.8 byte accesses would result in 4.8*32 = 155.0 bytes of cache data   
          transfers per request, to maximize L1TEX cache performance. Check the Source Counters section for             
          uncoalesced global loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 26.48%                                                                                          
          The memory access pattern for loads from L1TEX to L2 is not optimal. The granularity of an L1TEX request to   
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 1.4 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced loads and try to minimize how many cache lines need to be accessed per memory         
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.2164%                                                                                         
          The memory access pattern for stores from L1TEX to L2 is not optimal. The granularity of an L1TEX request to  
          L2 is a 128 byte cache line. That is 4 consecutive 32-byte sectors per L2 request. However, this kernel only  
          accesses an average of 1.0 sectors out of the possible 4 sectors per cache line. Check the Source Counters    
          section for uncoalesced stores and try to minimize how many cache lines need to be accessed per memory        
          request.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 45.09%                                                                                          
          The memory access pattern for loads from device memory causes 49,240,820 sectors to be read from DRAM, which  
          is 1.2x of the 39,614,628 sectors which cause a miss in the L2 cache. The DRAM fetch granularity for read     
          misses in L2 is 64 bytes, i.e. the lower or upper half of an L2 cache line. Try changing your access pattern  
          to make use of both sectors returned by a DRAM read request for optimal usage of the DRAM throughput. For     
          strided memory reads, avoid strides of 64 bytes or larger to avoid moving unused sectors from DRAM to L2.     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        23.88
    Issued Warp Per Scheduler                        0.24
    No Eligible                            %        76.12
    Active Warps Per Scheduler          warp         7.13
    Eligible Warps Per Scheduler        warp         0.32
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 3.604%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this kernel each scheduler only      
          issues an instruction every 4.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 8 warps per scheduler, this kernel allocates an average of    
          7.13 active warps per scheduler, but only an average of 0.32 warps were eligible per cycle. Eligible warps    
          are the subset of active warps that are ready to issue their next instruction. Every cycle with no eligible   
          warp results in no instruction being issued and the issue slot remains unused. To increase the number of      
          eligible warps, avoid possible load imbalances due to highly different execution durations per warp.          
          Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.            

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.87
    Warp Cycles Per Executed Instruction           cycle        29.87
    Avg. Active Threads Per Warp                                20.29
    Avg. Not Predicated Off Threads Per Warp                    18.72
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 3.604%                                                                                          
          On average, each warp of this kernel spends 20.7 cycles being stalled waiting for a scoreboard dependency on  
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 69.5% of the total average of 29.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 9.908%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This kernel achieves an average of 20.3 threads being active per cycle. This is further reduced    
          to 18.7 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1068929.84
    Executed Instructions                           inst    171028774
    Avg. Issued Instructions Per Scheduler          inst   1068981.37
    Issued Instructions                             inst    171037019
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 117161
    Registers Per Thread             register/thread              26
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block        byte/block               0
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    Threads                                   thread        14996608
    Waves Per SM                                              366.13
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           16
    Block Limit Registers                 block           16
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            8
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        89.35
    Achieved Active Warps Per SM           warp        28.59
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 3.604%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (89.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.11
    Branch Instructions              inst     19141534
    Branch Efficiency                   %        68.81
    Avg. Divergent Branches                   16445.46
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 58.56%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 60857602 excessive sectors (59% of the    
          total 103860683 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source      
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

